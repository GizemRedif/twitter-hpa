services:

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Zookeeper'ın 2181 portunda TCP bağlantısı kabul edip etmediğini kontrol eder. (nc ile)
      # Servisin ayağa kalkıp portu dinlemeye başladığını doğrulamak için kullanılır.
      # Kafka gibi bağımlı servislerin daha güvenli başlatılmasını sağlar. 
      # (Kafka’nın Zookeeper 2181 portunu dinlemeye başlamadan önce başlatılmasını kısmen engeller.)
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "2181" ]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # Kafka topic'lerini otomatik oluşturur
  # tweets.raw: 3 partition, 7 gün retention, cleanup.policy=delete
  # tweets.alert: 1 partition, 1 gün retention (1 Partition nedeni: Alert'ler sırayla okunmalıdır. Verim önemsiz, latency önemlidir.)
  # tweets.metrics: 2 partition, 3 gün retention
  init-kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: init-kafka
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo 'Waiting for Kafka to be ready...'
      cub kafka-ready -b kafka:29092 1 30

      echo 'Creating tweets.raw topic...'
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --topic tweets.raw --partitions 3 --replication-factor 1 --config retention.ms=604800000 --config cleanup.policy=delete

      echo 'Creating tweets.alert topic...'
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --topic tweets.alert --partitions 1 --replication-factor 1 --config retention.ms=86400000

      echo 'Creating tweets.metrics topic...'
      kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --topic tweets.metrics --partitions 2 --replication-factor 1 --config retention.ms=259200000

      echo 'Topics created!'
      kafka-topics --list --bootstrap-server kafka:29092
      "
    restart: "no"

  # Merkezi Confluent Schema Registry: AVRO şema yönetimi için kullanılır.
  # İç Ağ Adresi (Konteynerler arası): http://schema-registry:8081
  # Dış Ağ Adresi (Senin bilgisayarından): http://localhost:8083
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8083:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/subjects" ]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-producer:
    build: ./kafka_producer
    container_name: kafka-producer
    depends_on:
      schema-registry:
        condition: service_healthy
    # Python loglarının (print vb.) terminalde anlık olarak görünmesini sağlar.
    environment:
      PYTHONUNBUFFERED: "1"
    restart: "no"

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - ./postgres-init.sql:/docker-entrypoint-initdb.d/init-metrics.sql:ro

  mongo:
    image: mongo:7
    container_name: mongo
    ports:
      - "27018:27017"
    volumes:
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro

  # Hazır image kullanmak yerine kendimiz build ediyoruz (Dockerfile kullanarak)
  mongo-alert-consumer:
    build: ./mongo_alert_consumer
    container_name: mongo-alert-consumer
    depends_on:
      schema-registry:
        condition: service_healthy
      mongo:
        condition: service_started
    environment:
      PYTHONUNBUFFERED: "1"
    restart: on-failure

  # Hazır image kullanmak yerine kendimiz build ediyoruz (Dockerfile kullanarak)
  mongo-raw-consumer:
    build: ./mongo_raw_consumer
    container_name: mongo-raw-consumer
    depends_on:
      schema-registry:
        condition: service_healthy
      mongo:
        condition: service_started
    environment:
      PYTHONUNBUFFERED: "1"
    restart: on-failure

  # Hazır image kullanmak yerine kendimiz build ediyoruz (Dockerfile kullanarak)
  pg-metrics-consumer:
    build: ./pg_metrics_consumer
    container_name: pg-metrics-consumer
    depends_on:
      schema-registry:
        condition: service_healthy
      postgres:
        condition: service_started
    environment:
      PYTHONUNBUFFERED: "1"
    restart: on-failure

  # Yukarıdaki consumerlar için:
  # docker-compose up komutu verildiğinde Docker, önce o klasörlerden imageleri build eder
  # Sonra o imajlardan konteynerleri yaratır.
  # En son depends_on kısmına bakarak, önce Schema Registry'nin "sağlıklı" (healthy) olmasını bekler, sonra senin consumer'larını çalıştırır.

  airflow:
    image: apache/airflow:2.7.1
    container_name: airflow
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    command: >
      bash -c " airflow db init && airflow users create
        --username admin
        --password admin
        --firstname admin
        --lastname admin
        --role Admin
        --email admin@example.com &&
      airflow webserver & airflow scheduler "

  # ============================================================
  # Flink JobManager (master node)
  # ============================================================
  # İşleri koordine eden ana node. Web UI 8082 portunda çalışır.
  # Dockerfile ile Maven build yapılır → fat JAR otomatik eklenir.
  # docker-entrypoint-override.sh ile job otomatik submit edilir.
  # Böylece "docker compose up --build" tek komutla her şey başlar.
  flink-jobmanager:
    build: ./flink-tweets-stream
    container_name: flink-jobmanager
    ports:
      - "8082:8081"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
    # Özel entrypoint: JobManager başlat → REST API bekle → job submit et
    entrypoint: [ "/bin/bash", "/opt/flink/usrlib/docker-entrypoint-override.sh" ]
    depends_on:
      schema-registry:
        condition: service_healthy
      init-kafka:
        condition: service_completed_successfully

  # ============================================================
  # Flink TaskManager (worker node)
  # ============================================================
  # Gerçek veri işleme bu node'da yapılır.
  # JobManager ile aynı image'ı kullanır (JAR + bağımlılık tutarlılığı için).
  # Task slot sayısı: 2 → bu worker aynı anda 2 paralel görev çalıştırabilir. (Task)
  flink-taskmanager:
    build: ./flink-tweets-stream
    container_name: flink-taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2    
    command: taskmanager
    depends_on:
      - flink-jobmanager
